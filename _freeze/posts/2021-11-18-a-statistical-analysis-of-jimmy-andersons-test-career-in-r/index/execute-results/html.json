{
  "hash": "e4307f9bc85bcdcaed6356d831094614",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"A Statistical Analysis of Jimmy Andersons Test Batting Career in R\"\ndate: \"2021-08-03\"\ncategories: [cricket, data science, r]\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rstan)\nlibrary(lubridate)\n\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nbatting <- readRDS(\"data/batting.rds\")\n```\n:::\n\n\n# Introduction\n\nJames (Jimmy) Anderson is well known as one of the best ever pace bowlers to have played the game. His record speaks for itself - the most ever caps for an English test cricketer, the most ever wickets for a fast bowler in the history of test cricket, the only player to take 1000 first class wickets in the 21st century. Whilst he is known for his bowling achievements, in this article I will take a look at his batting throughout his test career.\n\nHis test career spans over 18 years, making his debut in May 2003 and still playing as of current (July 2021). For the duration of his career, it's fair to say that batting has not been his strong point - he has usually been found at number 11 in the order. That's not to say he's an awful batsman, we've seen far worse batsmen during his career; when he and Monty Panesar were playing in the same team he would usually be given a promotion up to number 10. And despite the plethora of bowling records, he has a few batting records to go with it: he has the 5th most innings in test cricket before a duck, and he has the 3rd highest score by a number 11.\n\nThis article will be quite technical. I will be using the R programming language throughout, initially to perform some exploratory data analysis, and then later to create and compare mathematical models. These models are necessary so that we can understand how his batting has changed over time.\n\n# Exploratory Data Analysis\n\nFirstly let's take a glimpse at the data that we have.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatting %>% head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 Ã— 4\n   innings_no date       score dismissed\n        <int> <date>     <dbl> <lgl>    \n 1          1 2003-05-22     4 FALSE    \n 2          2 2003-06-05    12 FALSE    \n 3          3 2003-07-24     0 FALSE    \n 4          4 2003-07-31    21 FALSE    \n 5          5 2003-07-31     4 FALSE    \n 6          6 2003-08-14     0 FALSE    \n 7          7 2003-08-14     2 TRUE     \n 8          8 2003-08-21     0 FALSE    \n 9          9 2003-08-21     0 FALSE    \n10         10 2003-09-04     0 FALSE    \n```\n\n\n:::\n:::\n\n\nSo we can see that we have the innings number, the date, the score and whether or not he was dismissed for each innings in test cricket that Jimmy Anderson has played.\n\nTo begin our analysis let's plot the distribution of Jimmy's batting scores.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatting %>%\n  ggplot(aes(x = score, fill = dismissed)) +\n  geom_bar() +\n  scale_fill_discrete(name = \"Dismissed?\") +\n  scale_x_continuous(name = \"Score\") +\n  scale_y_continuous(name = \"Count\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/bar chart jimmy scores-1.png){width=672}\n:::\n:::\n\n\nSo despite not scoring a single duck (that is zero runs and out) in his first 54 innings, it is now his highest score when he is out. The scores look to follow a fairly geometric shape - this seems reasonable because you can think of a player having a constant probability of being dismissed before they score another run. Obviously this assumption does have some issues - you can see that Jimmy has been out for a score of 4 more often than a score of 3, and this is because scoring 4 runs through a single shot is quite common.\n\nWe are interested in how his batting has changed over time, so let's plot his batting scores against the innings number.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatting %>%\n  ggplot(aes(x = innings_no, y = score, col = dismissed)) +\n  geom_point() +\n  scale_y_continuous(breaks = seq(0, 90, by = 10), minor_breaks = NULL, name = \"Score\") +\n  scale_x_continuous(name = \"Innings Number\") +\n  scale_color_discrete(name = \"Dismissed?\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/runs by innings plot-1.png){width=672}\n:::\n:::\n\n\nWe can see that magnificent 81 at Trent Bridge in 2014 he scored standing out about half way through his career.\n\nColloquially, it is said that Jimmy's batting has deteriorated over time. [This excellent video](https://www.youtube.com/watch?v=p6aCJH2XhCo&ab_channel=JarrodKimber) by Jarrod Kimber explains how his batting, particularly against pace bowlers, has deteriorated as he has got older (and how he now only has one scoring shot which is the reverse sweep). The graph above supports that this is the case - just look at the number of times he passes 10 and larger scores in the first half of his career compared to more recently.\n\nWhilst this graph has a lot of data points and therefore has a lot of variance meaning it can be difficult to see any patterns, lets cluster the data to see if that adds clarity. Lets start by looking at his batting average by year.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatting %>%\n  group_by(year = year(date)) %>%\n  summarise(\n    ave = sum(score) / sum(dismissed)\n  ) %>%\n  ggplot(aes(x = year, y = ave)) +\n  geom_point() +\n  scale_x_continuous(name = \"Year\") +\n  scale_y_continuous(name = \"Batting Average\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/batting average by year-1.png){width=672}\n:::\n:::\n\nWhilst at first it might not again be initially obvious that there is clear pattern, we can see that Jimmy Anderson has averaged over 10 in 6 of his 18 calendar years (he didn't play a game in 2005) as a professional cricketer, and the most recent of these was in 2014. So he hasn't averaged over 10 in any of the past 7 years, but he averaged more than 10 in 4 of his first 7 years.\n\nLet's look into this further by calculating the rolling batting average for Anderson, where we take the nearest 25 innings either side to calculate this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblock_size <- 25\nmoving_batting_ave_df <- tibble(\n  innings_no = numeric(),\n  ave = numeric()\n)\nfor (i in (block_size + 1):(nrow(batting) - block_size)) {\n  moving_batting_ave <- batting %>%\n    filter(\n      innings_no >= i - block_size,\n      innings_no <= i + block_size\n    ) %>%\n    summarise(ave = sum(score) / sum(dismissed)) %>%\n    pull(ave)\n  moving_batting_ave_df <- moving_batting_ave_df %>%\n    add_row(\n      innings_no = i,\n      ave = moving_batting_ave\n    )\n}\nmoving_batting_ave_df %>%\n  ggplot(aes(x = innings_no, y = ave)) +\n  geom_point() +\n  scale_x_continuous(name = \"Innings Number\") +\n  scale_y_continuous(name = \"Moving Batting Average\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/moving batting average-1.png){width=672}\n:::\n:::\n\n\nWe can see that Jimmy Anderson's moving batting average has decreased quite a bit from when he started, from ~15 when he began his career to ~6 currently. We can see that this has mostly followed a decreasing shape for his whole career - the only exception being a slight increase in the mid part of his career, which was mostly fuelled by that 81 he scored.\n\n# Mathematics\n\nIn order to provide an in depth analysis of Jimmy's batting over time, I think it is necessary to construct a model of his batting performance over time. I will make a fairly simple model by assuming that the runs scored comes from a geometric distribution. As I have previously said, we have to make the following two assumptions:\n\n(i) If you are not dismissed before scoring another run then you score one extra run.\n(ii) The probability of being dismissed before scoring another run is constant, for a given time $t$.\n\nIt is clear that assumption (i) is false and the data is not generated this way because of the rules of cricket, and therefore a more sophisticated model would take this into account and model the underlying process more carefully. The second assumption is also probably false - batting in cricket becomes a lot easier after you have faced a number of deliveries because you have got more used to the bowlers and the conditions.\n\nWe also have to deal with times where he is not dismissed. We can think of these times as a censored observation - the innings ended and therefore we don't know what score he would have ended up on, but we know he would have scored at least the score that he was on at the time.\n\nSo mathematically speaking, we will design out model as follows:\n\n$$\n\\alpha_{i} \\sim Geom(\\theta_{t[i]}) \\\\\nscore_{i} = \n\\begin{cases}\n\\alpha_{i}, \\text{if dismissed}, \\\\\n\\{0, \\dots, \\alpha_{i}\\}, \\text{if not dismissed}\n\\end{cases}\n$$\n\n# Modelling\n\nTo start off with our model, I will define some cut off points for our training set (and also the warm up for the exponential weighted model below):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwarm_up_cut_off_year <- 2009\ntrain_cut_off_year <- 2016\n```\n:::\n\n\n## Exponential Weighted Moving Average\n\nThe first model that I will construct will be an exponential weighted moving mean. I will take each timestep as an individual innings, and optimize the weight such that the log likelihood is maximised. I do this in a slightly weird way where I recalculate the weighting parameter for each new observation in the test set - the reason I do this becomes clearer once I describe the state space model below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"src/exponential_weighted_moving_average.R\")\n\nwarm_up_cut_off_year <- 2009\ntrain_cut_off_year <- 2016\n\nwarm_up_cut_off <- batting %>%\n  filter(year(date) <= warm_up_cut_off_year) %>%\n  nrow\n\ntrain_cut_off <- batting %>%\n  filter(year(date) <= train_cut_off_year) %>%\n  nrow\n\nweights <- numeric(nrow(batting))\nfor (i in (train_cut_off+1):nrow(batting)) {\n  train <- batting %>%\n    filter(innings_no < i)\n  weights[i] <- get_optimal_weight(\n    warm_up_cut_off,\n    train$innings_no,\n    train$score,\n    !train$dismissed\n  )\n}\n\nprint(paste0(\"The optimal weight is: \", weights[nrow(batting)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The optimal weight is: 0.0157578544617485\"\n```\n\n\n:::\n:::\n\n\nWe can now use our calculated weights for each time point to calculate our predicted parameters for the test set, which we will then plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparams_by_timestep <- numeric(nrow(batting))\nfor (i in (train_cut_off+1):nrow(batting)) {\n  params_by_timestep[i] <- get_params_by_timestep(\n    weights[i],\n    warm_up_cut_off = warm_up_cut_off,\n    timestep_vec = batting$innings_no,\n    score_vec = batting$score,\n    not_out_vec = !batting$dismissed\n  )[i]\n}\n\nweighted_moving_ave_preds_by_innings_df <- tibble(\n  innings_no  = 1:max(batting$innings_no),\n  param = params_by_timestep\n) %>%\n  filter(innings_no > train_cut_off)\n\nweighted_moving_ave_preds_by_innings_df %>%\n  ggplot(aes(x = innings_no, y = param)) +\n  geom_point() +\n  scale_x_continuous(name = \"Innings Number\") +\n  scale_y_continuous(name = \"Parameter\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/parameter plot by innings number-1.png){width=672}\n:::\n:::\n\n\nI will now construct a model in the same way as above, but instead of taking each timestep as an individual innings, I will only take the timesteps as individual years.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweights <- numeric(max(year(batting$date)))\nfor (i in (train_cut_off_year+1):max(year(batting$date))) {\n  train <- batting %>%\n    filter(year(date) < i)\n  weights[i] <- get_optimal_weight(\n    warm_up_cut_off_year,\n    year(train$date),\n    train$score,\n    !train$dismissed\n  )\n}\n\nprint(paste0(\"The optimal weight is: \", weights[i]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The optimal weight is: 0.268949564172078\"\n```\n\n\n:::\n:::\n\n\nAnd lets again calculate our predicted parameters for the test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparams_by_timestep <- numeric(nrow(batting))\nfor (i in (train_cut_off_year+1):max(year(batting$date))) {\n  params_by_timestep[i] <- get_params_by_timestep(\n    weights[i],\n    warm_up_cut_off = warm_up_cut_off_year,\n    timestep_vec = year(batting$date),\n    score_vec = batting$score,\n    not_out_vec = !batting$dismissed\n  )[i]\n}\n\nweighted_moving_ave_preds_by_year_df <- tibble(\n  year  = 1:max(year(batting$date)),\n  param = params_by_timestep\n) %>%\n  filter(year > train_cut_off_year) \n\nweighted_moving_ave_preds_by_year_df %>%\n  ggplot(aes(x = year, y = param)) +\n  geom_point() +\n  scale_x_continuous(name = \"Year\") +\n  scale_y_continuous(name = \"Parameter\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/parameter plot by year-1.png){width=672}\n:::\n:::\n\n\n## State Space Model\n\nI will now construct a different type of time series model, called a state space model. We will let the parameter of the geometric distribution be the state, and then let this value evolve over time. We will let the state evolve through the equation $\\theta_{t+1} \\sim \\mathcal{N(\\theta_{t}, \\sigma)}$.\n\nI have written this model in a language called [Stan](https://mc-stan.org/), a probabilistic programming language for statistical inference. I display the model below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwriteLines(readLines(\"stan_models/model.stan\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunctions {\n  real geometric_lpmf(int y, real theta) {\n    return log(theta) + y * log(1 - theta);\n  }\n  real geometric_lccdf(int y, real theta) {\n    return (y + 1) * log(1 - theta);\n  }\n}\n\ndata {\n  int<lower=1> num_outs;\n  int<lower=1> num_not_outs;\n  int<lower=1> ntimesteps;\n  int<lower=1, upper=ntimesteps> timestep_out[num_outs];\n  int<lower=1, upper=ntimesteps> timestep_not_out[num_not_outs];\n  int<lower=0> score_out[num_outs];\n  int<lower=0> score_not_out[num_not_outs];\n}\n\nparameters {\n  real initial_param;\n  real unscaled_param[ntimesteps-1];\n  real<lower=0> sigma;\n}\n\ntransformed parameters {\n  real param[ntimesteps];\n  real transformed_param[ntimesteps];\n  param[1] = initial_param;\n  for (k in 2:ntimesteps) {\n    param[k] = param[k-1] + unscaled_param[k-1] * sigma;\n  }\n  transformed_param = inv_logit(param);\n}\n\nmodel {\n  for (i in 1:num_outs) {\n    target += geometric_lpmf(score_out[i] | transformed_param[timestep_out[i]]);\n  }\n  for (i in 1:num_not_outs) {\n    target += geometric_lccdf( (score_not_out[i] - 1) | transformed_param[timestep_not_out[i]]);\n  }\n  initial_param ~ normal(-2.25, 0.5);\n  unscaled_param ~ std_normal();\n  sigma ~ std_normal();\n}\n```\n\n\n:::\n:::\n\n\nNow, it can be difficult to fit a state space model with too many parameters, so I will only run this model where each timestep is a year. Below I load in the model and do statistical inference.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- stan_model(\"stan_models/model.stan\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nhash mismatch so recompiling; make sure Stan code ends with a blank line\n```\n\n\n:::\n\n```{.r .cell-code}\ntrain <- batting %>%\n  filter(year(date) <= train_cut_off_year)\n\nstandata <- list(\n  num_outs = sum(train$dismissed),\n  num_not_outs = sum(!train$dismissed),\n  ntimesteps = max(year(train$date)) - 2002,\n  timestep_out = year(train$date[train$dismissed]) - 2002,\n  timestep_not_out = year(train$date[!train$dismissed]) - 2002,\n  score_out = train$score[train$dismissed],\n  score_not_out = train$score[!train$dismissed]\n)\n\nfit <- sampling(model, standata, seed = 1, iter = 10000, control = list(adapt_delta = 0.995))\n```\n:::\n\n\nThe first thing to do after MCMC is to check the fit. Let's start by printing the output of the fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=10000; warmup=5000; thin=1; \npost-warmup draws per chain=5000, total post-warmup draws=20000.\n\n                         mean se_mean   sd    2.5%     25%     50%     75%\ninitial_param           -2.48    0.00 0.26   -3.07   -2.64   -2.46   -2.31\nunscaled_param[1]        0.08    0.01 0.93   -1.75   -0.54    0.08    0.71\nunscaled_param[2]        0.08    0.01 0.94   -1.78   -0.55    0.07    0.70\nunscaled_param[3]        0.10    0.01 0.93   -1.72   -0.52    0.09    0.73\nunscaled_param[4]        0.00    0.01 0.91   -1.79   -0.60    0.00    0.62\nunscaled_param[5]       -0.30    0.01 0.93   -2.06   -0.94   -0.33    0.31\nunscaled_param[6]        0.36    0.01 0.88   -1.37   -0.22    0.36    0.93\nunscaled_param[7]        0.87    0.01 0.92   -1.03    0.29    0.90    1.49\nunscaled_param[8]       -0.06    0.01 0.93   -1.79   -0.70   -0.09    0.56\nunscaled_param[9]        0.25    0.01 0.87   -1.47   -0.31    0.25    0.83\nunscaled_param[10]       0.00    0.01 0.86   -1.66   -0.57   -0.01    0.56\nunscaled_param[11]      -0.28    0.01 0.88   -1.91   -0.88   -0.31    0.29\nunscaled_param[12]       0.50    0.01 0.90   -1.38   -0.08    0.53    1.10\nunscaled_param[13]       0.02    0.01 0.91   -1.77   -0.59    0.01    0.63\nsigma                    0.20    0.00 0.16    0.01    0.08    0.16    0.28\nparam[1]                -2.48    0.00 0.26   -3.07   -2.64   -2.46   -2.31\nparam[2]                -2.46    0.00 0.27   -3.06   -2.61   -2.44   -2.29\nparam[3]                -2.45    0.00 0.29   -3.09   -2.60   -2.42   -2.28\nparam[4]                -2.42    0.00 0.25   -2.97   -2.57   -2.41   -2.27\nparam[5]                -2.43    0.00 0.23   -2.92   -2.55   -2.41   -2.29\nparam[6]                -2.52    0.00 0.23   -3.08   -2.65   -2.49   -2.36\nparam[7]                -2.45    0.00 0.19   -2.87   -2.55   -2.42   -2.32\nparam[8]                -2.23    0.00 0.22   -2.59   -2.38   -2.26   -2.13\nparam[9]                -2.29    0.00 0.19   -2.68   -2.40   -2.29   -2.17\nparam[10]               -2.23    0.00 0.17   -2.55   -2.35   -2.24   -2.13\nparam[11]               -2.24    0.00 0.17   -2.56   -2.35   -2.25   -2.14\nparam[12]               -2.33    0.00 0.19   -2.76   -2.43   -2.31   -2.21\nparam[13]               -2.20    0.00 0.20   -2.56   -2.33   -2.22   -2.08\nparam[14]               -2.20    0.00 0.23   -2.63   -2.35   -2.22   -2.06\ntransformed_param[1]     0.08    0.00 0.02    0.04    0.07    0.08    0.09\ntransformed_param[2]     0.08    0.00 0.02    0.04    0.07    0.08    0.09\ntransformed_param[3]     0.08    0.00 0.02    0.04    0.07    0.08    0.09\ntransformed_param[4]     0.08    0.00 0.02    0.05    0.07    0.08    0.09\ntransformed_param[5]     0.08    0.00 0.02    0.05    0.07    0.08    0.09\ntransformed_param[6]     0.08    0.00 0.02    0.04    0.07    0.08    0.09\ntransformed_param[7]     0.08    0.00 0.01    0.05    0.07    0.08    0.09\ntransformed_param[8]     0.10    0.00 0.02    0.07    0.08    0.09    0.11\ntransformed_param[9]     0.09    0.00 0.02    0.06    0.08    0.09    0.10\ntransformed_param[10]    0.10    0.00 0.02    0.07    0.09    0.10    0.11\ntransformed_param[11]    0.10    0.00 0.02    0.07    0.09    0.10    0.11\ntransformed_param[12]    0.09    0.00 0.01    0.06    0.08    0.09    0.10\ntransformed_param[13]    0.10    0.00 0.02    0.07    0.09    0.10    0.11\ntransformed_param[14]    0.10    0.00 0.02    0.07    0.09    0.10    0.11\nlp__                  -367.95    0.06 3.59 -375.46 -370.24 -367.80 -365.49\n                        97.5% n_eff Rhat\ninitial_param           -2.02 12192    1\nunscaled_param[1]        1.90 21408    1\nunscaled_param[2]        1.94 22661    1\nunscaled_param[3]        1.94 21908    1\nunscaled_param[4]        1.82 23783    1\nunscaled_param[5]        1.57 18775    1\nunscaled_param[6]        2.09 22763    1\nunscaled_param[7]        2.59 15050    1\nunscaled_param[8]        1.85 16593    1\nunscaled_param[9]        1.96 22638    1\nunscaled_param[10]       1.73 23524    1\nunscaled_param[11]       1.55 19090    1\nunscaled_param[12]       2.21 17907    1\nunscaled_param[13]       1.85 24889    1\nsigma                    0.61  5534    1\nparam[1]                -2.02 12192    1\nparam[2]                -1.95 19298    1\nparam[3]                -1.89 19299    1\nparam[4]                -1.93 23715    1\nparam[5]                -2.00 22523    1\nparam[6]                -2.16 10851    1\nparam[7]                -2.13 15148    1\nparam[8]                -1.71 11187    1\nparam[9]                -1.89 21349    1\nparam[10]               -1.86 17812    1\nparam[11]               -1.88 19119    1\nparam[12]               -2.00 18853    1\nparam[13]               -1.76 14021    1\nparam[14]               -1.71 18471    1\ntransformed_param[1]     0.12 12966    1\ntransformed_param[2]     0.12 20119    1\ntransformed_param[3]     0.13 19828    1\ntransformed_param[4]     0.13 23666    1\ntransformed_param[5]     0.12 22634    1\ntransformed_param[6]     0.10 10415    1\ntransformed_param[7]     0.11 14900    1\ntransformed_param[8]     0.15 11247    1\ntransformed_param[9]     0.13 21272    1\ntransformed_param[10]    0.13 17920    1\ntransformed_param[11]    0.13 19189    1\ntransformed_param[12]    0.12 19046    1\ntransformed_param[13]    0.15 14017    1\ntransformed_param[14]    0.15 18314    1\nlp__                  -361.29  4160    1\n\nSamples were drawn using NUTS(diag_e) at Tue May 21 14:11:04 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\nSo the `Rhat` value is 1 for everything, which should mean that the chains are well mixed. We can see that the effective sample size is lowest for the `sigma` parameter. Let's take a look at the traceplot for this and a few other parameters, just to check that the model has fit okay.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraceplot(fit, \"sigma\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/traceplot sigma-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntraceplot(fit, \"initial_param\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/traceplot initial param-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntraceplot(fit, \"unscaled_param[1]\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/traceplot unscaled param-1.png){width=672}\n:::\n:::\n\n\n\nThe traceplot for all these parameters look good and the chains are well mixed. Therefore we can conclude that our chains have probably converged.\n\nWe now want to calculate our predicted parameters for each year. To do this, we need to retrain the model for each year with the updated dataset (a technique called filtering is often used to do this instead of retraining the model).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits <- list()\nfor (i in (train_cut_off_year+1):max(year(batting$date))) {\n  train <- batting %>%\n    filter(year(date) < i)\n  standata <- list(\n    num_outs = sum(train$dismissed),\n    num_not_outs = sum(!train$dismissed),\n    ntimesteps = max(year(train$date)) - 2002,\n    timestep_out = year(train$date[train$dismissed]) - 2002,\n    timestep_not_out = year(train$date[!train$dismissed]) - 2002,\n    score_out = train$score[train$dismissed],\n    score_not_out = train$score[!train$dismissed]\n  )\n  fits[[as.character(i)]] <- sampling(\n    model,\n    standata,\n    seed = 1,\n    iter = 10000,\n    control = list(adapt_delta = 0.995),\n    open_progress = FALSE,\n    show_messages = FALSE,\n    refresh = -1\n  )\n}\n```\n:::\n\n\nSince we have refitted the models on each extra bit of training data, we now have our predicted parameters. Lets plot them to see what they look like.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparams_by_timestep <- numeric(nrow(batting))\nfor (i in (train_cut_off_year+1):max(year(batting$date))) {\n  params_by_timestep[i] <- extract(fits[[as.character(i)]])[[\"transformed_param\"]][,i-2003] %>% mean\n}\n\nstate_space_preds_df <- tibble(\n  year  = 1:max(year(batting$date)),\n  param = params_by_timestep\n) %>%\n  filter(year > train_cut_off_year)\n\nstate_space_preds_df %>%\n  ggplot(aes(x = year, y = param)) +\n  geom_point() +\n  scale_x_continuous(name = \"Year\") +\n  scale_y_continuous(name = \"Parameter\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/parameter plot-1.png){width=672}\n:::\n:::\n\n\n# Model Comparison\n\nNow that I have predicted parameters from all the models, I can calculate their log loss scores and compare them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- batting %>%\n  filter(year(date) > train_cut_off_year)\n\nweighted_mov_ave_preds_by_innings <- numeric(nrow(test))\nweighted_mov_ave_preds_by_year <- numeric(nrow(test))\nstate_space_preds <- numeric(nrow(test))\n\nfor (i in 1:nrow(test)) {\n  weighted_mov_ave_preds_by_innings[i] <- weighted_moving_ave_preds_by_innings_df %>%\n    filter(innings_no == test$innings_no[i]) %>%\n    pull(param)\n  weighted_mov_ave_preds_by_year[i] <- weighted_moving_ave_preds_by_year_df %>%\n    filter(year == year(test$date[i])) %>%\n    pull(param)\n  state_space_preds[i] <- state_space_preds_df %>%\n    filter(year == year(test$date[i])) %>%\n    pull(param)\n}\n\nprint(get_neg_log_lik(weighted_mov_ave_preds_by_innings, test$score, !test$dismissed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 69.13824\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(get_neg_log_lik(weighted_mov_ave_preds_by_year, test$score, !test$dismissed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 68.97544\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(get_neg_log_lik(state_space_preds, test$score, !test$dismissed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 68.76052\n```\n\n\n:::\n:::\n\nSo we can see that the state space model gives us the best predictions on this test set.\n\n# Future Work\n\nA few improvements that could be done for this model in the future:\n\n* Address assumption (i) - a possible way of doing this is to model the number of balls faced as an intermediate step, and then runs scored from this.\n* Address assumption (ii) by assuming that the probability of being dismissed changes as his innings progresses.\n* Include other key factors such as the strength of the opposition and the match conditions.\n\n# Conclusion\n\nLets take our best model and look at what it says about Jimmy Anderson's batting strength over time:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstandata <- list(\n  num_outs = sum(batting$dismissed),\n  num_not_outs = sum(!batting$dismissed),\n  ntimesteps = max(year(batting$date)) - 2002,\n  timestep_out = year(batting$date[batting$dismissed]) - 2002,\n  timestep_not_out = year(batting$date[!batting$dismissed]) - 2002,\n  score_out = batting$score[batting$dismissed],\n  score_not_out = batting$score[!batting$dismissed]\n)\n\nfit <- sampling(\n  model,\n  standata,\n  seed = 1,\n  iter = 10000,\n  control = list(adapt_delta = 0.995),\n  open_progress = FALSE,\n  show_messages = FALSE,\n  refresh = -1\n)\n\nparams_by_timestep <- numeric(nrow(batting)+1)\nmodel_params <- extract(fit)\nfor (i in 2004:2022) {\n  params_by_timestep[i] <- mean(model_params[[\"transformed_param\"]][,i-2003])\n}\n\nstate_space_preds_df <- tibble(\n  year  = 1:(max(year(batting$date))+1),\n  param = params_by_timestep\n) %>%\n  filter(year > 2003) %>%\n  mutate(exp_ave = (1 - param) / param)\n\nstate_space_preds_df %>%\n  ggplot(aes(x = year, y = exp_ave)) +\n  geom_point() +\n  scale_x_continuous(name = \"Year\") +\n  scale_y_continuous(name = \"Parameter\", limits = c(7, 12))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/jimmy expected average over time-1.png){width=672}\n:::\n:::\n\n\nIn conclusion, the two things that stand out to me are:\n\n* It seems to show that from 2010 Jimmy's batting got a lot worse - it would be interesting to know what caused quite a large and sudden change.\n* His batting has undergone a steady decline towards the back end of his career, as is often said by commentators and pundits.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}